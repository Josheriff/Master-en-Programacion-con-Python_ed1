# Pruebas (_testing_)

En el ciclo de desarrollo, es natural correr algún programa que compruebe que
nuestros algoritmos hacen lo que deben hacer. Esto son las pruebas, o _tests_.

Probar un software implica, además de comprobar que funciona correctamente,
poder automatizar este proceso.

En los últimos años, las pruebas automatizadas han adquirido una importancia
notable, no sólo como herramienta de verificación, sino también como
**herramienta de diseño** y es común encontrar metodologías que ponen especial
énfases en las pruebas como el **diseño dirigido por tests**
(o [_TDD_](https://en.wikipedia.org/wiki/Test-driven_development), por sus
siglas en inglés) o el **diseño dirigido por comportamiento**
(o [_BDD_](https://en.wikipedia.org/wiki/Behavior-driven_development)).

Ambas técnicas refuerzan la idea del diseño _top-down_, donde **primero
definimos qué queremos hacer y luego implementamos la funcionalidad**. _BDD_ va
un paso más allá, introduciendo una metodología para escribir pruebas de forma
que personas no técnicas puedan definirlas.

* [Rediscovering TDD](https://www.quora.com/Why-does-Kent-Beck-refer-to-the-rediscovery-of-test-driven-development-Whats-the-history-of-test-driven-development-before-Kent-Becks-rediscovery)*
* [Introducing BDD](https://dannorth.net/introducing-bdd/)*
* [Specialized tooling support](https://en.wikipedia.org/wiki/Behavior-driven_development#Specialized_tooling_support) for BDD
* [Python BDD framework comparison](https://automationpanda.com/2019/04/02/python-bdd-framework-comparison/)

## Anatomía de una prueba

Por lo general, una prueba consta de las siguientes partes:

1. **Nombre**. Describe qué prueba el _test_.
2. **Expectativa**. Describe qué resultado espera el _test_.
3. **Preparación del entorno**. Configura el entorno con las condiciones
iniciales.
4. **Ejecución de la prueba**. Ejercita el código que queremos probar.
5. **Comprobación del resultado**. Compara el resultado contra algún valor de
referencia esperado.
6. **Limpieza del entorno**. Restaura el entorno a las condiciones en las que
se encontraba antes de la preparación.

Las herramientas de pruebas de los distintos lenguajes intentan que la
escritura de las partes 1, 2, 4 y 5 sea ágil y tratan de extraer en común la
configuración de los test.

## El módulo `unittest`

Python ofrece el módulo `unittest` para la creación de pruebas.

1. Crea un proyecto nuevo y añade el módulo `test_fibonacci.py`. Dentro,
escribe:

    ```python
    import unittest
    from fibonacci import fibonacci

    class TestFibonacci(unittest.TestCase):

        def test_base_case_0(self):
            """Should return 1 for index 0."""
            expected_result = 1
            actual_result = fibonacci(0)
            self.assertEqual(expected_result, actual_result)


    if __name__ == '__main__':
        unittest.main()
    ```

    El módulo se ha convertido en una colección de casos (_test suite_), la
    clase es un caso de estudio (_test case_) y los métodos que comiencen por
    `test*` en pruebas.

1. Además, a la misma altura crea un fichero `fibonacci.py` y escribe:

    ```python
    def fibonacci(index):
        if index in (0, 1):
            return 1

        return fibonacci(index - 1) + fibonacci(index - 2)
    ```

2. Ahora ejecuta el siguiente comando en la raíz del proyecto:

    ```bash
    $ python3 -m unittest
    ```

    Esto descubrirá automáticamente los test que tengas buscando clases que
    hereden de `unittest.TestCase` en aquellos ficheros que comiencen por
    `test*`.

    El resultado debería ser similar a:

    ```bash
    $ python -m unittest
    .
    ----------------------------------------------------------------------
    Ran 1 test in 0.000s

    OK
    ```

3. Si queremos que los _tests_ arrojen algo más de información, podemos usar:

    ```bash
    $ python -m unittest -v
    ```

    La opción `-v` configura el lanzador de pruebas o _test runner_ en modo
    detallado y la salida luce ahora:

    ```bash
    $ python -m unittest -v
    test_base_case_0 (test_fibonacci.TestFibonacci)
    Should return 1 for index 0. ... ok

    ----------------------------------------------------------------------
    Ran 1 test in 0.000s

    OK
    ```

4. Por lo general, las pruebas se situan en una carpeta `tests` que replican
el árbol de directorios y módulos del paquete principal del proyecto. Así que
si tu proyecto tiene esta pinta...

    ```
    .
    └── awesomelib
        ├── __init__.py
        ├── fibonacci.py
        └── utils
            ├── __init__.py
            └── network.py
    ```

    Junto con el directorio de tests debería ser algo como:

    ```
    .
    ├── awesomelib
    │   ├── __init__.py
    │   ├── fibonacci.py
    │   └── utils
    │       ├── __init__.py
    │       └── network.py
    └── tests
        ├── __init__.py
        ├── test_fibonacci.py
        └── utils
            ├── __init__.py
            └── test_network.py
    ```

    Es importante que los directorios en `tests` sean paquetes de Python, así
    que deben incluir los ficheros `__init__.py` convenientes.

6. Añade un segundo y tercer test a `TestFibonacci` para cubrir el otro caso
base y el caso recursivo (por ejemplo para el índice `20` el resultado es
`10946`). Por ejemplo, el segundo caso base queda:

    ```python
    def test_base_case_1(self):
        """Should return 1 for index 1."""
        expected_result = 1
        actual_result = fibonacci(1)
        self.assertEqual(expected_result, actual_result)
    ```

7. A veces algunos tests son muy parecidos, como les ocurre a los dos del caso
base. Podemos agruparlos en subpruebas con el método `subTest`:

    ```python
    def test_base_cases(self):
        """Should return 1 for indices 0 and 1."""
        expected_result = 1
        for index in (0, 1):
            with self.subTest(index=index):
                actual_result = fibonacci(index)
                self.assertEqual(expected_result, actual_result)
    ```

    Corre los tests para ver cómo el informador (_test reporter_) presenta
    el hecho de estar ejecutando subpruebas.

    Ahora modifica el código fuente para que falle intencionadamente cuando
    reciba el índice `0`. ¿Cómo se informa? Restaura el código cuando termines.

### Comprobaciones

Fíjate en el método `self.assertEqual`. Python permite muchos tipos de
comprobaciones que puedes encontrar en la documentación. Cuando quieras hacer
una comprobación, trata de encontrar el método más semántico posible.

1. Para que compruebes la importancia de utilizar siempre el método más
semántico, considera la siguiente prueba. Puedes añadirla al caso de estudio
`TestFibonacci` o crear uno nuevo para este tipo de experimentos con la API:

    ```python
    import unittest

    class TestUnittestAPI(unittes.TestCase):

        def test_assert_in(self):
            self.assertIn(1, ['a', 'b', 'c'])
    ```

    Esta prueba falla indicando:

    ```
    AssertionError: 1 not found in ['a', 'b', 'c']
    ```

    Sin embargo, modifica ahora el test para usar una comprobación más general:

    ```python
    def test_assert_in(self):
        self.assertTrue(1 in ['a', 'b', 'c'])
    ```

    ¿Qué observas? ¿Qué mensaje es más informativo?

2. Existe otro tipo de comprobaciones, las comprobaciones contextuales. Por
ejemplo, añade un test a `TestFibonacci` para comprobar el funcionamiento de la
función cuando recibe un índice negativo:

    ```python
    def test_negative_index(self):
        """Should raise a ValueError exception."""
        with self.assertRaises(ValueError):
            _ = fibonacci(-1)
    ```

    El estado actual de la función `fibonacci` no hace que falle el test
    sino que sea erróneo, indicando que se alcanza el límite de la recursión.
    Discutiremos este resultado más adelante. Por el momento, modifica la
    función para que devuelva `None` cuando reciba un índice negativo y observa
    cómo falla el _test_.

    ```
    AssertionError: ValueError not raised
    ```

    Cambia ahora la función para que lance la excepción `ValueError` que
    esperábamos.

3. Modifica la función `fibonacci` para que el mensaje de la excepción
`ValueError` sea `'The index is negative.'`. Podemos recuperar la excepción
y comprobar el mensaje mediante:

    ```python
    def test_negative_index(self):
        """Should raise a ValueError exception."""
        with self.assertRaises(ValueError) as cm:
            _ = fibonacci(-1)

        self.assertIn('is negative', str(cm.exception))
    ```

* [Lista de comprobaciones](https://docs.python.org/3/library/unittest.html#assert-methods)*

### Resultados de un _test_

Hemos visto 3 tipos de resultados hasta el momento: éxito, fracaso y error.

El éxito ocurre cuando el test termina. El fracaso ocurre si algún método
`assert*` falla. De hecho, lo que ocurre es que el método lanza una excepción
del tipo `AssertionError`. El error ocurre cuando se lanza una excepción que
**no es `AssertionError`**.

1. Podemos omitir (_skip_) un test usando el decorador `@unittest.skip()`,
junto con una razón. Añade este test al caso de estudio de experimentos `TestUnittestAPI`:

    ```python
    @unittest.skip('Teaching purposes')
    def test_skip_decorator(self):
        """Should not run."""
        self.assertTrue(False)  # would fail if run
    ```

    Si ejecutas las pruebas, el resultado será:

    ```bash
    $ python -m unittest
    ...s
    ----------------------------------------------------------------------
    Ran 4 tests in 0.000s

    OK (skipped=1)
    ```

    Fíjate que el informe indica una prueba omitida. El modo detallado
    indica las razones por las que ignorar el test. Compruébalo.

2. Lo normal no es usar `@unittest.skip()` sino `@unittest.skipIf()` que
permite evaluar una condición y omitir el test en caso de que la condición
sea verdadera. Por ejemplo:

    ```python
    from sys import version_info
    @unittest.skipIf(version_info.minor < 6, 'Python version too old.')
    def test_skipIf_decorator(self):
        """Should run in Python 3.6 and above only."""
        ...
    ```

3. Es también posible que durante un tiempo, se espere que cierto código falle.
Para indicar esta situación se utiliza el decorador
`@unittest.expectedFailure`:

    ```python
    @unittest.expectedFailure
    def test_intended_failure(self):
        self.assertTrue(False)
    ```

    El informe señala en este caso que el test falló... como se esperaba:

    ```bash
    $ python -m unittest
    .x..
    ----------------------------------------------------------------------
    Ran 4 tests in 0.001s

    OK (expected failures=1)
    ```

La siguiente tabla resume los posibles resultados de las pruebas:

| Resultado | Cuándo se produce                          | Símbolo |
|-----------|--------------------------------------------|---------|
|Éxito      | El test termina.                           | `.`     |
|Fallo      | Falla un método `assert*`.                 | `F`     |
|Error      | Se produce una excepción.                  | `E`     |
|Omisión    | Se usan los decoradores `skip*`.           | `s`     |
|Fallo esperado | Se usa el decorador `expectedFailure`. | `x`     |

### Configuración

Los _tests_ pueden configurarse, por ejemplo, si es necesario falsear algunos
objetos con los que las funcionalidad que queremos probar tiene que
relacionarse. O si tenemos que preparar una base de datos de una determinada
forma antes de probar el código que manipula dicha base de datos.

Para ello podemos usar los métodos `setUp()` y `tearDown()` que se ejecutan
**justo antes y después de cada test, respectivamente**. Por ejemplo, considera
el siguiente listado con funcionalidad y _test_, todo incluído:

```python
import unittest

class DataBaseController:

    def __init__(self, db=None):
        self._db = db if db is not None else set()

    def add(self, name):
        self._db.add(name)

    def remove(self, name):
        self._db.remove(name)


class TestDataBaseController(unittest.TestCase):

    def setUp(self):
        self._db = set(['Salva'])
        self._controller = DataBaseController(self._db)

    def test_add_method(self):
        """Should add a new item to the database."""
        self._controller.add('Ziltoid')
        self.assertIn('Ziltoid', self._db)
        self.assertEqual(2, len(self._db))

    def test_remove_method(self):
        """Should remove an existent item from the database."""
        self._controller.remove('Salva')
        self.assertNotIn('Salva', self._db)
        self.assertEqual(0, len(self._db))
```

### Nuevas comprobaciones

Es conveniente que la fase de comprobación de un _test_, lo sea de una sola
cosa. Si tu listado de comprobaciones comienza a crecer o incluye lógica,
considera crear una nueva comprobación. Por ejemplo:

```python
import unittest

class TestFileUtils(unittest.TestCase):

    def assertAllFilesVisible(self, filenames):
        """Raises ``AssertionError`` if some filename starts with `.`."""
        for name in filenames:
            if name.startswith('.'):
                raise AssertionError(f'Filename "{name}" is a hidden file.')

    def test_all_files_visible(self):
        """Should fail pointing to ``.DS_Store``."""
        self.assertAllFilesVisible(['a.jpg', 'b.png', '.DS_Store'])
```

### Falseadores (_mocks_)

Es inevitable, tarde o temprano tu código interactuará con código externo y,
dependiendo del tipo de prueba que estemos llevando a cabo, querremos controlar
el resultado del código ajeno.

Aquí es donde entran en juego los falseadores o _mocks_.

1. Supon que queremos probar la siguiente funcionalidad. Crea un fichero
`library.py` y añade las siguientes funciones:

    ```python
    def categories_by_book(db):
        """
        Parse a text in the format:

        book name = category 1, category 2, category 3...
        book name = category 1, category 2, category 3...

        into a dictionary of books and the list of its categories.
        """
        result = {}
        for line in map(str.strip, db.split('\n')):
            if line:
                book_name, categories_format = map(str.strip, line.split('='))
                categories = list(map(str.strip, categories_format.split(',')))
                result[book_name] = categories

        return result

    def books_by_category(db):
        """
        Parse a text in the format:

        book name = category 1, category 2, category 3...
        book name = category 1, category 2, category 3...

        into a dictionary of categories and the set of books of this category.
        """
        from collections import defaultdict
        index = categories_by_book(db)
        reverse_index = defaultdict(set)
        for book in index:
            for category in index[book]:
                reverse_index[category].append(book)
        return reverse_index
    ```

    No podemos comprobar la función `books_by_category` en aislamiento porque
    depende de `categories_by_book`. ¿Qué se te ocurre que puedas hacer?

2. La solución pasa por inyectar temporalmente una versión de
`categories_by_book` que funcione como nosotros queramos. Crea un fichero
`test_library.py` y añade el siguiente código:

    ```python
    import unittest
    import library

    class TestLibrary(unittest.TestCase):

        def setUp(self):
            self._db = """
    Dracula = adventure, horror
    De la Terre à la Lune = sci-fi, adventure
    """

        def test_books_by_category(self):
            """
            Should return a dict with categories and lists of books in those
            categories.
            """
            categories_by_book = {
                'Dracula': { 'adventure', 'horror' },
                'De la Terre à la Lune': { 'sci-fi', 'adventure' }
            }

            from unittest.mock import patch
            with patch('library.categories_by_book',
                       return_value=categories_by_book):
                result = library.books_by_category(self._db)

            expected_result = {
                'adventure': { 'Dracula', 'De la Terre à la Lune' },
                'horror': { 'Dracula' },
                'sci-fi': { 'De la Terre à la Lune' }
            }

            self.assertEqual(expected_result, result)
    ```

    El test es más largo pero no más complicado. Para empezar fijamos las
    precondiciones con el resultado de `categories_by_book()` en forma de
    diccionario con el resultado de la dependencia que queremos controlar.

    Ahora, con `patch()`, parcheamos la función de la que depende la
    comprobación para que devuelva el valor controlado que fijamos
    anteriormente.

    Con el parche aplicado, ejecutamos el código que queremos probar. Al final
    del contexto, el parche se elimina automáticamente.

    Luego comprobamos contra el valor esperado.

3. Una variación permite inspeccionar el parche en sí mismo para comprobar,
por ejemplo, que la función `categories_by_book` fue llamada con el mismo
parámetro que se le pasó a la función objetivo. Modifica el test para recoger
el objeto de contexto de la sentencia `with`:

    ```python
   def test_books_by_category(self):
        """
        Should return a dict with categories and lists of books in those
        categories.
        """
        categories_by_book = {
            'Dracula': { 'adventure', 'horror' },
            'De la Terre à la Lune': { 'sci-fi', 'adventure' }
        }

        from unittest.mock import patch
        with patch('library.categories_by_book',
                   return_value=categories_by_book) as mock:
            result = library.books_by_category(self._db)

        expected_result = {
            'adventure': { 'Dracula', 'De la Terre à la Lune' },
            'horror': { 'Dracula' },
            'sci-fi': { 'De la Terre à la Lune' }
        }

        mock.assert_called_once_with(self._db)
        self.assertEqual(expected_result, result)
    ```

4. A nuestra función `fibonacci` le pasa un poco lo mismo. Al ser recursiva,
depende de sí misma. Probar un escenario controlando las precondiciones, supone
controlar los casos base. ¿Se te ocurre cómo podrías falsear `fibonacci`
para tan sólo controlar los casos base y hacerlos siempre correctos?

El módulo `unittest.mock` es muy potente y cuesta un rato acostumbrarse a
cómo utilizarlo. Lo más importante a recordar de cara al parcheo de objetos,
funciones o métodos es indicar **dónde ocurrirá el parche**. Las utilidades
de parcheo esperan cadenas que indiquen las rutas de lo que tienen que
parchear. **Los parches sólo tendrán efecto allí.**

* [Documentación del módulo `unittest.mock`](https://docs.python.org/3/library/unittest.mock.html)

## El módulo `doctest`

Python implementa una forma de "programación literaria"
([_literate programming_](https://en.wikipedia.org/wiki/Literate_programming))
que permite incrustar tests en la documentación de un módulo. Veremos un
ejemplo rápido:

1. Modifica la función `fibonacci` para que luzca así:

    ```python
    def fibonacci(index):
        """
        Calculate the index-th number of the fibonacci sequence.

        >>> fibonacci(0)
        1
        >>> fibonacci(1)
        1
        >>> fibonacci(2)
        2
        >>> fibonacci(3)
        3
        >>> fibonacci(4)
        5

        The n-th number of the fibonacci sequence is the sum of the two
        previous fibonacci numbers.

        Trying to get the fibonacci number of a negative value is an error:

        >>> fibonacci(-1)
        Traceback (most recent call last):
        ...
        ValueError: The index is negative.
        """
        if index < 0:
            raise ValueError('The index is negative.')

        if index in (0, 1):
            return 1

        return fibonacci(index - 1) + fibonacci(index - 2)
    ```

2. Ahora, ejecuta el siguiente comando desde una terminal:

    ```bash
    $ python -m doctest -v fibonacci.py
    ```

    La salida es larga y descriptiva:

    ```
    $ python -m doctest -v fibonacci.py
    Trying:
        fibonacci(0)
    Expecting:
        1
    ok
    Trying:
        fibonacci(1)
    Expecting:
        1
    ok
    Trying:
        fibonacci(2)
    Expecting:
        2
    ok
    Trying:
        fibonacci(3)
    Expecting:
        3
    ok
    Trying:
        fibonacci(4)
    Expecting:
        5
    ok
    Trying:
        fibonacci(-1)
    Expecting:
        Traceback (most recent call last):
        ...
        ValueError: The index is negative.
    ok
    1 items had no tests:
        fibonacci
    1 items passed all tests:
    6 tests in fibonacci.fibonacci
    6 tests in 2 items.
    6 passed and 0 failed.
    Test passed.
    ```

3. La sintáxis de los tests es sencilla, se introduce la prueba con `>>>` y
a continuación se expresa el resultado deseado.

Los test en la documentación sirven para mantener documentación y pruebas
actualizadas.

* [Información completa del módulo `doctest`](https://docs.python.org/3.7/library/doctest.html)

## Tipos de pruebas

No todas las pruebas verifican las mismas cosas. Por ejemplo, lo que
has practicado hasta ahora son test unitarios, definidos a continuación además
de otros tipos de pruebas:

* **Los test unitarios** prueban funcionalidad en aislamiento, donde
controlamos todas las precondiciones de la ejecución.

* **Los _tests_ de integración** comprueban funcionalidades extremo a extremo,
cuando las funcionalidades inviduales han sido agregadas para componer una
utilidad mayor.

    ![En la imagen, una mano echa un cerrojo que asegura una puerta al marco de
    la misma; sin embargo, la puerta era deslizante lo que convierte al
    cerrojo en una solución inútils, una vez integrado en este
    contexto](./imgs/integration-vs-unit-test.gif)

* **Los _tests_ de regresion** forman una colección creciente de comprobaciones
para problemas encontrados y arreglados en producción, de forma que podamos
garantizar que nuevas versiones del software no re-introducen lo ya arreglado.

    Los _tests_ de regresión son, además, una **excelente herramienta de
    depuración**. Cuando encuentres un problema, **conviértelo en un test
    de regresión**: establece unas condiciones iniciales, llama a la
    funcionalidad y compara con el resultado esperado. Puedes poner un punto
    de interrupción en la llamada a la funcionalidad para obtener rápidamente
    un punto de acceso a las precondiciones deseadas.

* **Los _tests_ de propiedades** comprueban dominios enteros basándose en
las propiedades que debe cumplir una funcionalidad y no en el valor concreto.

    [Hypothesis](https://hypothesis.readthedocs.io/en/latest/) es un framework
    para realizar _tests_ de propiedad.
